# 用深度学习求解高维偏微分方程

## 摘要

解决高维偏微分方程（PDEs）的问题一直是一个极其困难的任务，长期以来被“维数灾难”这一著名问题所困扰。本文提出了一种基于深度学习的方法，可以处理一般的高维抛物型PDEs。为此，本文通过反向随机微分方程（BSDEs）重新表述PDEs，并通过神经网络来近似未知解的梯度，这种方法与深度强化学习中的策略函数具有相似性。包括非线性Black-Scholes方程、Hamilton-Jacobi-Bellman方程和Allen-Cahn方程的数值结果表明，所提出的算法在高维情况下，无论在准确性还是成本方面，都具有显著的效果。这为经济学、金融学、运筹学和物理学开辟了新的可能性，能够在这些领域中考虑所有参与者、资产、资源或粒子一起参与，而不是仅仅做出关于它们相互关系的假设。

## 第一章：引言

偏微分方程（PDEs）是自然问题建模中最为常见的工具之一。许多重要的物理问题天然地被表示为高维PDEs。以下是几个著名的例子：

1. 量子多体问题中的薛定谔方程。在这种情况下，PDE的维度大约是系统中电子或量子粒子的三倍。
2. 用于金融衍生品定价的非线性Black-Scholes方程，其中PDE的维度是所考虑的基础金融资产的数量。
3. 动态规划中的Hamilton-Jacobi-Bellman方程。在博弈论中，多主体的情况下，维度随着参与者的增加而线性增长。类似地，在资源分配问题中，维度随着设备和资源数量的增加而增加。

尽管这些PDE模型非常优雅，但由于“维数灾难”问题，它们的实际应用受到了极大限制：随着维度的增加，求解这些方程的计算成本呈指数增长。

此外，“维数灾难”在机器学习和数据分析中也是一个重要障碍。例如，非线性回归模型的复杂度随着维度的增加而呈指数级增长。在这两种情况下，我们面临的核心问题是如何在高维空间中表示或逼近一个非线性函数。传统的方法通过构建多项式、分段多项式、小波或其他基函数来表示函数，但这将不可避免地遇到维数灾难的问题。

近年来，一类新的技术——深度神经网络模型，在人工智能领域取得了显著的成功。尽管神经网络这一概念已有相当长时间，但近期的研究表明，深度神经网络在处理复杂数据集时表现得相当好。在函数表示方面，神经网络的特点是通过简单函数的组合来近似复杂函数。而传统的近似理论方法通常是加法形式的。数学上，有普适近似定理表明，单层神经网络可以近似一类广泛的函数，尽管我们仍缺乏理论框架来解释深度神经网络的超常效果。

尽管如此，深度神经网络在人工智能领域的实践成功鼓励我们将其应用到其他领域，尤其是在“维数灾难”仍然困扰着许多问题的领域。

在本文中，我们扩展了深度神经网络的应用，提出了一种通过深度学习求解大类高维非线性PDEs的策略。我们研究的PDE类型是（非线性）抛物型PDEs。具体包括Black-Scholes方程和Hamilton-Jacobi-Bellman方程。为此，我们利用反向随机微分方程（BSDEs）对这些PDEs进行重新表述，并通过深度神经网络近似解的梯度。这种方法在一定程度上与深度强化学习相似，BSDE扮演了基于模型的强化学习角色，而解的梯度则扮演了策略函数的角色。数值例子表明，所提出的算法在精度和计算成本方面都非常有效。

## 第二章：方法论

我们考虑一类常见的PDE，称为半线性抛物型PDE。此类PDE可以表示如下形式：

$$
\frac{\partial u}{\partial t}(t, x) + \frac{1}{2} \text{Tr}\left( \sigma \sigma^T(t, x) \text{Hess}_x u(t, x) \right) + \nabla u(t, x) \cdot \mu(t, x) + f\left(t, x, u(t, x), \sigma^T(t, x) \nabla u(t, x)\right) = 0
$$

其中，$t$ 和 $x$ 分别表示时间和空间（$d$-维）变量，$\mu$ 是已知的向量值函数，$\sigma$ 是已知的 $d \times d$ 矩阵值函数，$\sigma^T$ 表示 $\sigma$ 的转置，$\nabla u$ 和 $\text{Hess}_x u$ 分别是函数 $u$ 关于 $x$ 的梯度和海塞矩阵，$\text{Tr}$ 是矩阵的迹，$f$ 是已知的非线性函数。为简化起见，我们关心的是 $t = 0$ 时，空间位置为某个向量 $\xi \in \mathbb{R}^d$ 的解。

设 $W_t$ 是 $d$-维标准布朗运动，$X_t$ 是满足以下随机微分方程（SDE）的过程：

$$
X_t = \xi + \int_0^t \mu(s, X_s) ds + \int_0^t \sigma(s, X_s) dW_s
$$

那么，方程（1）对应的解 $u(t, X_t)$ 满足如下的反向随机微分方程（BSDE）：

$$
u(t, X_t) = u(0, X_0) - \int_0^t f\left(s, X_s, u(s, X_s), \sigma^T(s, X_s) \nabla u(s, X_s)\right) ds + \int_0^t \left[ \nabla u(s, X_s) \right]^T \sigma(s, X_s) dW_s
$$

我们将进一步在“材料和方法”部分详细解释这个BSDE的推导过程。

为了求解 $u(0, X_0)$，我们将 $u(0, X_0) \approx \theta u_0$，$\nabla u(0, X_0) \approx \theta \nabla u_0$ 作为模型中的参数，并将上述BSDE作为求解终端时间 $T$ 时 $u(t_N, X_{t_N})$ 值的工具。我们对方程（2）和（3）进行时间离散化。给定时间区间 $[0, T]$ 的划分：$0 = t_0 < t_1 < \dots < t_N = T$，我们使用简单的欧拉方法进行离散化：

$$
X_{t_{n+1}} - X_{t_n} \approx \mu(t_n, X_{t_n}) \Delta t_n + \sigma(t_n, X_{t_n}) \Delta W_n
$$

以及

$$
u(t_{n+1}, X_{t_{n+1}}) \approx u(t_n, X_{t_n}) - f\left(t_n, X_{t_n}, u(t_n, X_{t_n}), \sigma^T(t_n, X_{t_n}) \nabla u(t_n, X_{t_n})\right) \Delta t_n + \left[\nabla u(t_n, X_{t_n})\right]^T \sigma(t_n, X_{t_n}) \left(W_{t_{n+1}} - W_{t_n}\right)
$$

其中 $\Delta t_n = t_{n+1} - t_n$ 和 $\Delta W_n = W_{t_{n+1}} - W_{t_n}$。

基于这一时间离散化，路径 $\{ X_{t_n} \}_{0 \leq n \leq N}$ 可以通过（4）式进行简单地采样。接下来的关键步骤是近似每个时间步 $t_n = t_1, \dots, t_{N-1}$ 时，函数 $x \mapsto \sigma^T(t, x) \nabla u(t, x)$ 的值。为此，我们使用一个多层前馈神经网络：

$$
\sigma^T(t_n, X_{t_n}) \nabla u(t_n, X_{t_n}) = (\sigma^T \nabla u)(t_n, X_{t_n}) \approx (\sigma^T \nabla u)(t_n, X_{t_n} | \theta_n)
$$

这里，$\theta_n$ 表示神经网络的参数，网络的目标是近似函数 $x \mapsto \sigma^T(t, x) \nabla u(t, x)$。

然后，将这些子网络组合起来，形成一个整体的深度神经网络。该网络以路径 $\{ X_{t_n} \}_{0 \leq n \leq N}$ 和布朗运动的增量 $\{ W_{t_n} \}_{0 \leq n \leq N}$ 作为输入数据，并给出最终的输出 $\hat{u}(\{ X_{t_n} \}_{0 \leq n \leq N}, \{ W_{t_n} \}_{0 \leq n \leq N})$，作为 $u(t_N, X_{t_N})$ 的近似。

为了优化网络参数，我们定义期望损失函数：

$$
l(\theta) = \mathbb{E} \left[ \left| g(X_{t_N}) - \hat{u} \left( \{ X_{t_n} \}_{0 \leq n \leq N}, \{ W_{t_n} \}_{0 \leq n \leq N} \right) \right|^2 \right]
$$

参数集为：$\theta = \{ \theta_{u_0}, \theta_{\nabla u_0}, \theta_1, \dots, \theta_{N-1} \}$。

我们可以使用类似于标准深度神经网络训练的随机梯度下降（SGD）算法来优化参数 $\theta$，在我们的数值例子中使用了 Adam 优化器【14】。如需更多关于深度神经网络训练的细节，请参见“材料和方法”部分。由于反向随机微分方程（BSDE）被作为核心工具，我们将上述方法称为深度BSDE方法。

## 第三章：例子

### 1. 非线性Black-Scholes方程与违约风险

金融衍生品交易中的一个关键问题是如何确定适当的公允价格。Black 和 Scholes 通过研究表明，金融衍生品的价格 $u$ 满足一个抛物型PDE，这个方程现在被称为 Black-Scholes 方程【15】。Black-Scholes 模型可以被扩展，以考虑真实市场中的一些重要因素，如违约证券、借贷利率与存款利率之间的差异、交易成本、模型参数的不确定性等（参见【16–20】）。这些因素中的每一个都会在定价模型中产生非线性贡献（参见【17, 21, 22】）。特别是，信用危机和当前的欧洲主权债务危机突显了在原始 Black-Scholes 模型中被忽略的最基本风险——违约风险【21】。

理想情况下，定价模型应考虑所有相关的标的资产，这会导致高维的非线性PDE。然而，由于“维数灾难”，现有的定价算法普遍无法处理这些问题。为了演示深度BSDE方法的有效性，我们研究了一个递归定价模型的特例，其中考虑了违约风险【16, 17】。我们考虑了一个基于100个标的资产的欧洲期权的公允价格，假设在此时没有违约发生。当期权发行人的违约发生时，期权持有者只会收到当前价值的一个比例 $\delta \in [0, 1)$。违约的发生通过一个泊松过程的首次跳跃时间来建模，其强度 $Q$ 是当前价值的递减函数，即当期权的价值较低时，违约发生的概率更高。价值过程可以通过以下PDE表示：

$$
\frac{\partial u}{\partial t}(t, x) + \mu_x \cdot \nabla u(t, x) + \frac{\sigma^2}{2} \sum_{i=1}^d |x_i|^2 \frac{\partial^2 u}{\partial x_i^2}(t, x) - (1 - \delta)Q(u(t, x))u(t, x) - Ru(t, x) = 0
$$

其中，$Q(y)$ 是一个分段线性函数，表示违约强度，具体形式为：

$$
Q(y) = 1_{(-\infty, v_h)}(y) \gamma_h + 1_{[v_l, \infty)}(y) \gamma_l + 1_{[v_h, v_l)}(y) \left[ \frac{(\gamma_h - \gamma_l)}{(v_h - v_l)} (y - v_h) + \gamma_h \right]
$$

在此情况下，我们选择了以下参数：$T = 1$，$\delta = 2/3$，$R = 0.02$，$\mu_x = 0.02$，$\sigma_x = 0.2$，$v_h = 50$，$v_l = 70$，$\gamma_h = 0.2$，$\gamma_l = 0.02$，并选择终端条件为 $g(x) = \min\{x_1, x_2, \dots, x_{100}\}$ 其中 $x = (x_1, \dots, x_{100}) \in \mathbb{R}^{100}$。

图1展示了 $\theta u_0$ 的均值和标准差，作为 $u(t=0, x = (100, \dots, 100))$ 的近似值，最终的相对误差为 0.46%。通过与没有考虑违约风险的定价模型（得到的价格为 $u(t=0, x=(100, \dots, 100)) \approx 60.781$）进行比较，深度BSDE方法在计算时能够严格纳入违约风险，从而实现了更精确的定价。

### 2. Hamilton-Jacobi-Bellman (HJB) 方程

“维数灾难”这一术语最早由 Richard Bellman 在动态规划的背景下提出【1】，如今在许多领域中都发挥了核心作用，如经济学、行为科学、计算机科学，甚至生物学，尤其是在智能决策问题中。在博弈论中，如果有多个参与者，每个参与者都必须解决一个高维的HJB类型方程，才能找到最优策略。在涉及多个实体的不确定性动态资源分配问题中，动态规划原理同样会导致一个高维的HJB方程【23】。直到最近，这些高维PDEs几乎无法求解。

在此，我们展示了深度BSDE方法在处理这些高维问题中的有效性。我们考虑一个经典的线性-二次-高斯（LQG）控制问题，在100维情况下，状态过程为：

$$
dX_t = 2 \sqrt{\lambda} m_t dt + \sqrt{2} dW_t
$$

其中，$X_t$ 是状态过程，$m_t$ 是控制过程，$\lambda$ 是表示控制“强度”的正数常量，$W_t$ 是标准布朗运动。我们的目标是通过控制过程来最小化成本泛函：

$$
J(\{m_t\}_{0 \leq t \leq T}) = \mathbb{E} \left[ \int_0^T \| m_t \|^2 dt + g(X_T) \right]
$$

此问题的HJB方程为：

$$
\frac{\partial u}{\partial t}(t, x) + \Delta u(t, x) - \lambda \| \nabla u(t, x) \|^2 = 0
$$

（参见，例如，Yong & Zhou【24】【第4章】）。解的值 $u(t, x)$ 在 $t = 0$ 时的值代表了从状态 $x$ 开始时的最优成本。通过应用 Itô 公式，可以证明该方程的确切解具有显式的公式：

$$
u(t, x) = - \frac{1}{\lambda} \ln \left( \mathbb{E} \left[ \exp \left( - \lambda g(x + \sqrt{2} W_{T-t}) \right) \right] \right)
$$

我们使用深度BSDE方法求解100维Hamilton-Jacobi-Bellman方程，在 $g(x) = \ln \left( \frac{1 + \| x \|^2}{2} \right)$ 时，解得的相对误差为0.17%，计算时间为330秒。

### 3. Allen-Cahn 方程

Allen-Cahn方程是一种反应扩散方程，广泛应用于物理学中，用作相分离和有序-无序转变的模型（参见【25】）。我们在100维空间中考虑一个典型的Allen-Cahn方程，具有“双井势”：

$$
\frac{\partial u}{\partial t}(t, x) = \Delta u(t, x) + u(t, x) - [u(t, x)]^3
$$

初始条件为 $u(0, x) = g(x)$，其中 $g(x) = \frac{1}{2 + 0.4 \| x \|^2}$，并应用时间变量变换 $t \mapsto T - t$（其中 $T > 0$），使得该方程转化为与上述形式相同，从而可以使用深度BSDE方法进行求解。

图3（a）展示了使用深度BSDE方法在求解100维Allen-Cahn方程时，解 $u(t=0.3, x = (0, \dots, 0))$ 的相对误差。最终的相对误差为0.30%，计算时间为647秒。图3（b）展示了通过深度BSDE方法计算的 $u(t, x = (0, \dots, 0))$ 在时间区间 $t \in [0, 0.3]$ 内的演变。

## 结论

本文提出的算法为多个领域开辟了新的可能性。例如，在经济学中，可以考虑多个不同的交互作用代理人一起参与，而不必使用“代表性代理人”模型。类似地，在金融领域，可以同时考虑所有参与的金融工具，而不必依赖于它们之间关系的任意假设。在运筹学中，可以直接处理涉及数百或数千个参与实体的情况，无需做出任意近似。

需要注意的是，尽管本文提出的方法具有一定的通用性，但我们目前尚无法处理量子多体问题，这是因为在量子问题中，保利排斥原理等问题使得处理过程变得异常复杂。

---

## 材料与方法

### BSDE 重表述

反向随机微分方程（BSDEs）与（非线性）抛物型PDE之间的联系在文献中已有广泛研究（参见【8, 9, 26, 27】）。特别是，马尔科夫型BSDEs为某些非线性抛物型PDE提供了非线性Feynman-Kac表示。设 $(\Omega, \mathcal{F}, \mathbb{P})$ 是一个概率空间，$W: [0, T] \times \Omega \to \mathbb{R}^d$ 是 $d$-维标准布朗运动，$\{ \mathcal{F}_t \}_{t \in [0, T]}$ 是由 $W_t$ 生成的自然滤波过程。考虑以下BSDE：

$$
X_t = \xi + \int_0^t \mu(s, X_s) ds + \int_0^t \sigma(s, X_s) dW_s
$$

以及

$$
Y_t = g(X_T) + \int_t^T f(s, X_s, Y_s, Z_s) ds - \int_t^T Z_s^T dW_s
$$

我们寻求一个适应 $\{ \mathcal{F}_t \}_{t \in [0, T]}$ 的解过程 $\{(X_t, Y_t, Z_t)\}_{t \in [0, T]}$，其中 $X_t \in \mathbb{R}^d$，$Y_t \in \mathbb{R}$，且 $Z_t \in \mathbb{R}^d$。在适当的正则性假设下，可以证明解的存在性和唯一性（参见【8, 26】）。

此外，我们有以下关系：对于所有 $t \in [0, T]$，几乎必然地（P-几乎确定）：

$$
Y_t = u(t, X_t) \quad \text{和} \quad Z_t = \sigma^T(t, X_t) \nabla u(t, X_t)
$$

因此，我们可以通过求解BSDE（16）和（17）来计算与（1）对应的 $u(0, X_0)$。具体来说，我们将（18）中的恒等式代入（17），并对其进行正向重写，从而得到（3）中的公式。

随后，我们对方程进行时间离散化，并使用神经网络近似空间梯度，最终得到对未知函数的逼近，如本文“方法”部分所介绍。

### 神经网络架构

在这一小节中，我们简要介绍了深度BSDE方法的神经网络架构。为了简化展示，我们将注意力集中在扩散系数 $\sigma$ 满足对所有 $x \in \mathbb{R}^d$，有 $\sigma(x) = I_d$ 的情况。在图4中，我们展示了深度BSDE方法的网络架构。需要注意的是，$\nabla u(t_n, X_{t_n})$ 是我们直接通过子网络进行逼近的变量，而 $u(t_n, X_{t_n})$ 是我们通过网络迭代计算的变量。该网络中包含三种类型的连接：

1. $X_{t_n} \to h_{1n} \to h_{2n} \to \cdots \to h_{Hn} \to \nabla u(t_n, X_{t_n})$，这是一个多层前馈神经网络，用于逼近时间步 $t = t_n$ 时的空间梯度。这个子网络的参数是我们优化的目标。
2. $(u(t_n, X_{t_n}), \nabla u(t_n, X_{t_n}), W_{t_{n+1}} - W_{t_n}) \to u(t_{n+1}, X_{t_{n+1}})$，这是一个前向迭代过程，给出了网络最终输出，作为 $u(t_N, X_{t_N})$ 的近似。这类连接中没有需要优化的参数。
3. $(X_{t_n}, W_{t_{n+1}} - W_{t_n}) \to X_{t_{n+1}}$，这是不同时间步之间的快捷连接，符合（4）和（6）的公式。同样，这类连接中没有需要优化的参数。

如果我们在每个子网络中使用 $H$ 个隐藏层，则整个网络将包含 $(H + 1)(N - 1)$ 层，且这些层涉及需要优化的自由参数。

图4展示了用于求解半线性抛物型PDE的网络架构，其中每个子网络有 $H$ 个隐藏层，每个时间步有 $N$ 个离散时间点。整个网络包含 $(H + 1)(N - 1)$ 层，涉及自由参数需要同时优化。

需要指出的是，如果我们对PDE解的值有兴趣的不仅仅是某个单一空间点 $\xi \in \mathbb{R}^d$，而是一个区域 $D \subset \mathbb{R}^d$ 内的值，我们同样可以应用深度BSDE方法。在这种情况下，我们选择 $X_0 = \xi$ 为一个非退化的 $D$-值随机变量，并使用两个额外的神经网络，通过 $\theta_{u_0}$ 和 $\theta_{\nabla u_0}$ 来逼近函数 $D \to \mathbb{R}$ 和 $D \to \mathbb{R}^d$ 中的 $u(0, x)$ 和 $\nabla u(0, x)$。

### 实现

我们详细描述了文中数值例子的实现过程。每个子网络是完全连接的，由4层组成（除下一小节中的例外），包括1个输入层（$d$-维）、2个隐藏层（每层为 $d+10$-维）和1个输出层（$d$-维）。我们选择使用ReLU作为激活函数。我们还在子网络中采用了批归一化技术【30】，每次线性变换之后并在激活之前应用此技术。这一技术可以加速训练，使得我们可以使用更大的步长并简化参数初始化。所有的参数都是通过正态分布或均匀分布初始化的，而没有预训练过程。

我们使用TensorFlow【31】实现我们的算法，并使用Adam优化器【14】来优化参数。Adam是SGD算法的一个变体，它基于对低阶矩的自适应估计。我们将对应的超参数设置为推荐值，批大小选择为64。在所有的数值例子中，我们通过5次独立运行不同随机种子来计算相对L1逼近误差的均值和标准差。

### 隐藏层数的影响

深度BSDE方法的准确性显然依赖于子网络近似中隐藏层的数量。为了测试这一影响，我们用不同数量的隐藏层解决一个反应扩散型PDE的高维版本（$d = 100$）。该PDE为：

$$
\frac{\partial u}{\partial t}(t, x) + \frac{1}{2} \Delta u(t, x) + \min\left\{ 1, (u(t, x) - u^*(t, x))^2 \right\} = 0
$$

其中 $u^*(t, x)$ 是显式的振荡解：

$$
u^*(t, x) = \kappa + \sin\left( \lambda \sum_{i=1}^d x_i \right) \exp\left( \frac{\lambda^2 d (t - T)}{2} \right)
$$

图表展示了随着隐藏层数增加，误差的变化情况。
