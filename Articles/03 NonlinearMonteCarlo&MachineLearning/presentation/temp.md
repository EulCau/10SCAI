这篇论文系统地综述了高维偏微分方程（PDE）的数值求解方法，特别是基于非线性蒙特卡洛和深度学习的两类算法。以下从论文结构、核心算法及数学公式的详细解释展开：

---

### **1. 引言与背景**
#### **高维PDE的挑战**
高维PDE广泛存在于控制理论、金融定价和量子力学等领域，例如：
1. **Hamilton-Jacobi-Bellman (HJB) 方程**（式1）：
   \[
   \frac{\partial u}{\partial t} + H(x, \nabla_x u) = 0
   \]
   - **含义**：描述动态规划中的最优值函数，\(H\)为哈密顿量，与状态\(x\)和梯度\(\nabla_x u\)相关。
   - **维度来源**：状态空间的维度，若原始控制问题由PDE描述，则HJB方程可能为无限维。

2. **非线性Black-Scholes方程**（式2）：
   \[
   \frac{\partial u}{\partial t} + \frac{1}{2}\sigma^2 \sum_{i=1}^d x_i^2 \frac{\partial^2 u}{\partial x_i^2} + r \langle \nabla_x u, x \rangle - ru + f(u) = 0
   \]
   - **含义**：金融衍生品定价方程，\(f(u)\)引入非线性项（如违约风险、交易成本）。
   - **维度来源**：标的资产数量\(d\)。

3. **多电子薛定谔方程**（式3）：
   \[
   i\frac{\partial u}{\partial t} = \Delta_x u + V(x)u
   \]
   - **维度来源**：电子数的3倍（每个电子的三维坐标）。

**核心问题**：传统方法（有限差分、有限元）因维度诅咒（CoD）无法处理高维问题，需复杂度为\(O(d^\alpha \varepsilon^{-\beta})\)的算法，其中\(\alpha, \beta\)为与维度无关的常数。

---

### **2. 核心算法**
#### **2.1 多级Picard迭代（MLP）**
**目标**：求解半线性抛物型PDE，例如半线性热方程（式18）：
\[
\frac{\partial u}{\partial t} = \Delta_x u + f(u)
\]
**关键步骤**：
1. **随机固定点方程**：通过Feynman-Kac公式将PDE转化为期望形式（式19）：
   \[
   u(t,x) = \mathbb{E}\left[ u(0, x+\sqrt{2}W_t) + \int_0^t f(u(s, x+\sqrt{2}W_{t-s})) ds \right]
   \]
   - \(W_t\)为标准布朗运动，公式将解表示为初始条件与非线性项的期望。

2. **Picard迭代**：递归构造近似解（式20）：
   \[
   \mathbf{u}_n = \Phi(\mathbf{u}_{n-1}) \quad \text{（通过蒙特卡洛方法近似期望）}
   \]
   - 利用多级蒙特卡洛（MLMC）思想，不同层级的采样精度不同，降低计算复杂度。

3. **复杂度分析**（定理3）：
   - 对误差\(\varepsilon\)，计算复杂度为\(O(d^c \varepsilon^{-3})\)，证明MLP方法克服了维度诅咒。

#### **2.2 Deep BSDE方法**
**核心思想**：将半线性PDE与倒向随机微分方程（BSDE）结合，通过神经网络逼近解和梯度。

**步骤**：
1. **BSDE形式**（式29-30）：
   \[
   \begin{cases}
   dX_t = \mu(t,X_t)dt + \sigma(t,X_t)dW_t \\
   Y_t = g(X_T) + \int_t^T f(s,X_s,Y_s,Z_s)ds - \int_t^T Z_s^T dW_s
   \end{cases}
   \]
   - \(Y_t\)对应解\(u(t,X_t)\)，\(Z_t\)对应梯度\(\sigma^T \nabla_x u\)。

2. **优化问题**：最小化终端条件误差（式40）：
   \[
   \mathcal{L}(\theta) = \mathbb{E}\left[ |g(X_T) - \hat{u}(X_T)|^2 \right]
   \]
   - 通过时间离散（式35-39）和神经网络参数化\(Y_0\)和\(Z_t\)，构建深度残差网络。

**数值实验**：
- **LQG控制问题**（式44）：通过HJB方程验证算法，解为对数期望形式（式45）。
- **非线性Black-Scholes模型**（式46）：对比MLP与Deep BSDE结果，显示高维问题的有效求解。

---

### **3. 变分与弱形式方法**
#### **3.1 Deep Ritz方法**
**原理**：基于变分原理，最小化能量泛函（式64）：
\[
I(u) = \int_\Omega \left( \frac{1}{2}|\nabla u|^2 - f u \right) dx
\]
- **神经网络表示**：用DNN逼近解\(u(x;\theta)\)，蒙特卡洛积分近似泛函。
- **边界条件处理**：通过惩罚项（式66）将Dirichlet条件融入损失函数。

#### **3.2 最小二乘法**
**目标**：最小化残差的\(L^2\)范数（式68）：
\[
J(u) = \int_\Omega \| Lu - f \|^2 \mu(dx)
\]
- 通用性强，但传统方法中条件数较差，结合DNN可提升高维适应性。

#### **3.3 Galerkin方法**
**弱形式**（式69）：
\[
a(u, \phi) = (f, \phi) \quad \text{（对任意测试函数\(\phi\)）}
\]
- **类比GAN**：生成器（解）与判别器（测试函数）对抗优化，类似Wasserstein GAN。

---

### **4. 数学理论**
#### **定理4（神经网络逼近能力）**
**结论**：存在DNN以多项式参数复杂度逼近半线性热方程的解（式76）：
\[
\left( \int_{[0,1]^d} |u_d(T,x) - \mathcal{R}(\mathfrak{u}_{d,\varepsilon})(x)|^2 dx \right)^{1/2} \leq \varepsilon
\]
- **关键假设**：初始条件可用DNN高效逼近，且解的增长受多项式控制。

**证明策略**：
1. 构造随机神经网络，其期望逼近PDE解。
2. 通过蒙特卡洛误差分析和压缩引理，确定确定性网络的存在性。

---

### **5. 结论与展望**
- **应用**：金融衍生品定价、随机控制、量子多体问题。
- **理论方向**：建立基于复杂度的PDE理论，分析解的表示复杂度而非传统正则性。
- **与强化学习的对比**：强化学习（如Bellman方程求解）与本文方法在模型利用和自适应性的差异。

---

### **数学公式总结**
| 公式 | 含义 |
|------|------|
| 式1  | HJB方程，描述最优控制的值函数 |
| 式19 | 通过Feynman-Kac公式将PDE转化为随机期望形式 |
| 式29-30 | BSDE系统，联系PDE解与随机过程\(Y_t, Z_t\) |
| 式74 | 多级Picard迭代，递归蒙特卡洛近似解 |
| 式64 | Deep Ritz方法的能量泛函，通过DNN最小化 |
| 式76 | DNN逼近PDE解的\(L^2\)误差界 |

---

这篇论文通过结合概率方法（如MLP）和深度学习（如Deep BSDE、Deep Ritz），为高维PDE提供了高效且理论保障的数值解法，同时开辟了基于复杂度的PDE分析新方向。