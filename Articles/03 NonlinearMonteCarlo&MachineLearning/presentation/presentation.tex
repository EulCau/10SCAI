\documentclass[aspectratio=169]{beamer}
\usetheme{Madrid}
\usefonttheme[onlymath]{serif}
\usepackage{ctex, amsmath, amsfonts, amssymb}
\usepackage{graphicx}

\def\dif{\mathinner{\mathrm{d}}\hphantom{\mskip-\thinmuskip}}

\title{高维 PDE 求解算法:\\从非线性蒙特卡罗到机器学习}
\author{刘行}
\date{\today}

\begin{document}

	\begin{frame}
		\titlepage
	\end{frame}

	\begin{frame}{大纲}
		\tableofcontents
	\end{frame}

	\section{引言与背景}
	\begin{frame}{引言与背景}
		\begin{itemize}
			\item 高维偏微分方程 (PDEs) 在控制理论, 金融工程, 量子力学等领域有重要应用
			\item 传统数值方法 (如有限差分, 有限元) 在高维情况下受``维数灾难''限制
			\item 本论文提出利用非线性蒙特卡罗方法和深度学习技术求解高维 PDE, 从而突破维数灾难
		\end{itemize}
	\end{frame}

	\section{主要方法与技术}
	\begin{frame}{深度 BSDE 方法 (Deep BSDE)}
		\begin{itemize}
			\item 将非线性抛物型 PDE
				\begin{equation*}
					\frac{\partial u}{\partial t} + \frac{1}{2} \mathrm{Tr}\left(\sigma\sigma^{\top}\,\mathrm{Hess}_x u\right) + \langle\nabla u, \mu\rangle + f\left(t, x, u, \sigma^{\top} \nabla u\right)=0, \quad u\left(T,x\right)=g\left(x\right)
				\end{equation*}
				与后向随机微分方程 (BSDE) 联系起来
			\item 通过 It\^{o} 引理可得:
				\begin{equation*}
					\begin{aligned}
						u\left(t, X_{t}\right) - u\left(0, X_{0}\right) = &-\int_{0}^{t} f\left(s, X_{s}, u\left(s, X_s\right), \sigma^{\top}\nabla u\left(s, X_{s}\right)\right) \dif s \\
						&+ \int_{0}^{t} \left(\nabla u\left(s, X_{s}\right)\right)^{\top}\sigma(s, X_{s}) \dif W_{s}.
					\end{aligned}
				\end{equation*}
		\end{itemize}
	\end{frame}

	\subsection{深度 BSDE 方法 (Deep BSDE)}
	\begin{frame}{深度 BSDE 方法 (Deep BSDE)}
		\begin{itemize}
			\item Pardoux 和 Peng 提出如果令 $Y_{t} = u\left(t, X_{t}\right), Z_{t} = \left[\sigma\left(t, X_{t}\right)\right]^{\top} \left(\nabla_{x} u \right)\left(t, X_{t}\right)$, 则随机过程 $\left(X_{t}, Y_{t}, Z_{t}\right) \in \mathbb{R}^{d} \times \mathbb{R} \times \mathbb{R}^d, t \in \left[0, T\right]$, 满足下面的 BSDE:
				\begin{equation*}
					\left\{\begin{array}{l}
						X_{t} = \xi + \int_{0}^{t} \mu\left(s, X_{s}\right) \dif s + \int_{0}^{t} \sigma\left(s, X_{s}\right) \dif W_{s} \\
						Y_{t} = g\left(X_{T}\right) + \int_{t}^{T} f\left(s, X_{s}, Y_{s}, Z_{s}\right) \dif s - \int_{t}^{T} \left(Z_{s}\right)^{\top} \dif W_{s}
					\end{array}\right.
				\end{equation*}
			\item 利用深度神经网络逼近未知函数: 例如用网络 $\psi_{0}$ 表示 $u(0, X_{0})$, 用子网络 $\phi_{n}$ 逼近 $Z_{t}$. 通过离散化时间构建网络, 将末端误差作为损失函数进行训练.
		\end{itemize}
	\end{frame}

	\begin{frame}{深度 BSDE 方法 (Deep BSDE)}
		\begin{itemize}
			\item 问题最终转化为
				\begin{align}
					\inf_{\psi_{0}, \left\{\phi_{n}\right\}_{n=0}^{N-1}} & \mathbb{E} \left\lvert g\left(X_{T}\right) - Y_{T}\right\rvert^{2}, \\
					s.t.\quad & X_{0} = \xi, \quad Y_{0} = \psi_{0}\left(\xi\right), \\
					& X_{t_{n+1}} = X_{t_{i}} + \mu\left(t_{n},X_{t_{n}}\right) \Delta t + \sigma\left(t_{n},X_{t_{n}}\right) \Delta W_{n}, \\
					& Z_{t_{n}} = \phi_{n}\left(X_{t_{n}}\right), \\
					& Y_{t_{n+1}} = Y_{t_{n}} - f\left(t_{n}, X_{t_{n}}, Y_{t_{n}}, Z_{t_{n}}\right) \Delta t + \left(Z_{t_{n}}\right)^{\top} \Delta W_{n}
				\end{align}
			\item 取误差函数为
				\begin{equation*}
					l\left(\theta\right) = \mathbb{E}\left[\left\lvert g\left(X_{t_{N}}\right) - \hat{u}\left(\left\{X_{t_{n}}\right\}_{0 \leq n \leq N}, \left\{W_{t_{n}}\right\}_{0 \leq n \leq N}\right)\right\rvert^2\right].
				\end{equation*}
				其中 $\hat{u}\left(\left\{X_{t_{n}}\right\}_{0 \leq n \leq N}, \left\{W_{t_{n}}\right\}_{0 \leq n \leq N}\right)$ 为网络的最后一个输出, 为 $u\left(t_{N}, X_{t_{N}}\right)$ 的近似
		\end{itemize}

	\end{frame}

	\section{高维控制问题}
	\begin{frame}{高维控制问题}
		\begin{itemize}
			\item 传统控制理论中, 最优控制问题的解往往受到维数灾难的限制
			\item 我们考虑下面的最优控制问题:
				\begin{equation*}
					\min_{u} g\left(x\left(T\right)\right) + \int_{0}^{T} L\left(t, x\left(t\right), u\left(t, x\left(t\right)\right)\right) \dif t
				\end{equation*}
				受限于动态系统
				\begin{equation*}
					\dot{x} = f\left(t, x, u\right) \quad x\left(0\right) = x_{0}
				\end{equation*}
			\item 对于开环控制, 仅沿最优路径定义控制函数 $u\left(t, x\right) = u^{*}\left(t, x^{*}\left(t\right)\right)$. 其自变量仅为时间, 维数灾难并不是主要问题
			\item 对于闭环控制, 控制函数 $u\left(t, x\right) = u^{*}\left(t, x\right)$ 在全局状态空间定义, 维数灾难成为主要问题
		\end{itemize}
	\end{frame}

	\begin{frame}{高维控制问题}
		\begin{itemize}
			\item Pontryagin 极小原理给出了最优控制问题的理论基础
			\item 令扩展哈密顿量为 $\tilde{H} = L + \lambda^{\top} f$, $u^{*}\left(t, x; \lambda\right) = \arg\min_{u\in U} \tilde{H}\left(t, x, \lambda, u\right)$ 则由 Pontryagin 极小原理, 下面的方程组有解
				\begin{equation*}
					\left\{
						\begin{array}{ll}
							\left.\dot{x} = \tilde{H}_{\lambda}\right|_{u = u^{*}}	& x\left(0\right) = x_{0}										\\
							\left.\dot{\lambda} = -\tilde{H}_{x}\right|_{u = u^{*}}	& \lambda\left(T\right) = \nabla g\left(x\left(T\right)\right)	\\
							\left.\dot{v} = -L\right|_{u = u^{*}}											& v\left(T\right) = g\left(x\left(T\right)\right)
						\end{array}
					\right.
				\end{equation*}
				其中, $x$ 为状态变量, $\lambda$ 为伴随变量, $\tilde{H}$ 为 Hamilton 函数
		\end{itemize}
	\end{frame}

	\begin{frame}{高维控制问题}
		\begin{itemize}
			\item 记
				\begin{equation*}
					V\left(t, x\right) = \inf_{u \in U}\{g\left(y\left(T\right)\right) + \int_{t}^{T}L\left(\tau, y, u\right) \dif \tau\}
				\end{equation*}
				其中 $\dot{y}\left(\tau\right) = f\left(\tau, y, u\right), y\left(t\right) = x$,
				\begin{equation*}
					H^{*}\left(t, x, \lambda\right) = \tilde{H}\left(t, x, \lambda, u^{*}\left(t, x; \lambda\right)\right)
				\end{equation*}
				则 $V$ 满足 HJB 方程 $V_{t} + H^{*}\left(t, x, V_{x}\right) = 0$, 且有终端条件 $V\left(T, x\right) = g\left(x\right)$ 通过数值方法求解 HJB 方程, 可以得到最优控制函数 $u^{*}$
		\end{itemize}
	\end{frame}

	\begin{frame}{高维控制问题}
		\begin{itemize}
			\item 想通过机器学习方法求解高维控制问题, 需要推广到所有初值条件. 于是考虑如下问题
				\begin{equation*}
					\min_{u} \mathbb{E}_{x_{0}\sim\mu}\left[g\left(x\left(T\right)\right) + \int_{0}^{T} L\left(t, x\left(t\right), u\left(t, x\left(t\right)\right)\right) \dif t \right]
				\end{equation*}
				\begin{equation*}
					\dot{x} = f\left(t, x, u\right),\quad x\left(0\right) = x_{0},\quad \mu = \frac{1}{Z}e^{-\beta V}
				\end{equation*}
			\item 为解决数据生成问题, 需要采用如下策略:
				\begin{enumerate}
					\item 求解前面的两点边值问题来生成训练数据
					\item 利用生成的数据训练神经网络, 以逼近值函数 $V$ 和最优控制函数 $u^{*}$
				\end{enumerate}
			\item 然而, 这个两点边值问题的求解也不简单, 具体可以采用 ``热启动'' 策略或者自适应采样方法.
		\end{itemize}
	\end{frame}

	\section{Ritz, Galerkin 和最小二乘法}
	\subsection{Ritz 方法}
	\begin{frame}{Deep Ritz Method}
		\begin{itemize}
			\item 考虑如下变分问题:
				\begin{equation*}
					\min_{u \in H}I\left(u\right), \quad I\left(u\right) = \int_{\Omega}\left(\frac{1}{2}\left\lvert\nabla u\left(x\right)\right\rvert^{2} - f\left(x\right)u\left(x\right)\right) \dif x
				\end{equation*}
			\item Deep Ritz 方法主要包括:
				\begin{enumerate}
					\item 使用 DNN 参数化试探函数
					\item 对 $I\left(u\right)$ 采取数值积分进行离散化
					\item 设计最终算法
				\end{enumerate}
			\item 处理边界条件时可能会遇到一些问题, 可以对泛函进行修正:
				\begin{equation*}
					I\left(u\right) = \int_{\Omega}\left(\frac{1}{2} u_{x}^{2} - fu\right) \dif x + \beta\int_{\partial\Omega}u^{2} \dif x
				\end{equation*}
				通常可选 $\beta = 500$.
		\end{itemize}
	\end{frame}

	\subsection{最小二乘法}
	\begin{frame}{最小二乘法}
		\begin{itemize}
			\item 考虑在区域 $\Omega \in \mathbb{R}^{d}$ 上求解 PDE
				\begin{equation*}
					Lu=f
				\end{equation*}
			\item 可等价的转化为变分问题
				\begin{equation*}
					\min_{u} J\left(u\right) = \int_{\Omega}\left\lVert Lu - f\right\rVert^{2}\mu\left(\dif x\right)
				\end{equation*}
				其中 $\mu$ 是 $\Omega$ 上选取的非退化易于采样的概率分布. 于是问题与前面变得类似.
		\end{itemize}
	\end{frame}

	\subsection{Galerkin 方法}
	\begin{frame}{Galerkin 方法}
		\begin{itemize}
			\item 考虑弱形式: 寻找 $u \in H_{1}$ 使得对任意 $\varphi \in H_{2}$ 有
				\begin{equation*}
					a\left(u, \varphi\right) = \left\langle Lu, \varphi\right\rangle = \left\langle f, \varphi\right\rangle
				\end{equation*}
			\item 通常进行分部积分使其仅涉及一阶导数.
			\item 类似 WGAN 方法, 可以将其改写为
				\begin{equation*}
					\min_{u \in H_{1}} \max_{\left\lVert\varphi\right\rVert _{H_{2} \leq 1}} \left(a\left(u, \varphi\right) - \left\langle f, \varphi\right\rangle\right)^{2}
				\end{equation*}
			\item 然而这种形式的公式很难求解
		\end{itemize}
	\end{frame}

	\subsection{多层皮卡德方法 (MLP)}
	\begin{frame}{多层皮卡德方法 (MLP)}
		\begin{itemize}
			\item 定理3主要提供了多层 Picard 近似方法 (MLP 方法) 在求解半线性热偏微分方程 (具有 Lipschitz 连续的非线性项) 时的误差估计和计算复杂度分析. 
			\item 假设 PDE 由半线性热方程
				\begin{equation*}
					\frac{\partial}{\partial t} u_{d}\left(t, x\right) = \Delta_{x} u_{d}\left(t, x\right) + f\left(u_{d}\left(t,x\right)\right)
				\end{equation*}
				给出, 其中 $u_{d}\left(t, x\right)$ 是精确解, $f\left(u\right)$ 是 Lipschitz 连续的非线性项.
			\item 设定精度阈值  $\varepsilon$, 则 MLP 方法所需的计算成本 $\mathfrak{C}_{d,\mathfrak{N}_{\varepsilon}, \mathfrak{N}_{\varepsilon}}$ 满足:
				\begin{equation*}
					\mathfrak{C}_{d,\mathfrak{N}_{\varepsilon}, \mathfrak{N}_{\varepsilon}} \leq cd^{c}\varepsilon^{-3},
				\end{equation*}
     			其中 $c$ 是一个与维数 $d$ 和精度 $\varepsilon$ 无关的常数. 这表明 MLP 方法的计算成本仅呈多项式增长, 相比于传统方法的指数增长 (即``维数灾难''), MLP 方法在一定程度上克服了维数灾难.
			\end{itemize}
		\end{frame}

		\begin{frame}{多层皮卡德方法 (MLP)}
			\begin{itemize}
			\item 该方法的 $L^2$ 误差估计满足:
				\begin{equation*}
					\left(\mathbb{E}\left[\left\lvert U^{d,0}_{\mathfrak{N}_{\varepsilon}, \mathfrak{N}_\varepsilon}\left(T, 0\right) - u_{d}\left(T, 0\right)\right\rvert^2\right]\right)^{\frac{1}{2}} \leq \varepsilon
				\end{equation*}
				这意味着, MLP 方法可以在计算成本受控的情况下, 将误差控制在 $\varepsilon$ 以内.
			\item 定理 3 的核心贡献
				\begin{enumerate}
					\item 提供了 MLP 方法对 PDE 逼近的严格误差分析, 证明了该方法能够在多项式计算复杂度下达到高精度解
					\item 明确了计算复杂度的上界, 即计算成本随维度 $d$ 线性增长, 随误差 $\varepsilon^{-3}$ 级增长
					\item 支持 MLP 方法在高维 PDE 计算中的可行性, 理论上证明了该方法在高维问题中比传统网格方法 (如有限差分, 有限元) 更具优势
				\end{enumerate}
			\item 总的来说, 定理 3 证明了 MLP 方法是一种在计算效率和逼近精度之间具有良好平衡的数值方法, 特别适用于高维 PDE 计算
		\end{itemize}
	\end{frame}

	\begin{frame}{神经网络近似逼近 PDEs 数值解的数学结果}
		\begin{itemize}
			\item 截至今日, 尚没有完整的严格数学分析能够证明 (或证伪) 以下猜想: 存在一种基于深度学习的近似方法能够在偏微分方程 (PDE) 的数值逼近中克服维度诅咒 (curse of dimensionality). 然而, 已经有一些数学结果证明, DNN 具备在不引入维度诅咒的条件下近似 PDE 解的能力.
			\item 定理 4 表明, 设 $u_{d}\left(t, x\right)$ 满足非线性热方程:
				\begin{equation*}
					\frac{\partial u_d}{\partial t} = \Delta_x u_d + f(u_d), \quad u_d(0,x)\text{初值}
				\end{equation*}
    			若初值可用 DNN 以多项式复杂度逼近, 则
				\begin{enumerate}
					\item 存在神经网络 $\mathfrak{u}_{d,\varepsilon}$, 其参数数量 $\mathcal{P}\left(\mathfrak{u}_{d,\varepsilon}\right)$ 至多以多项式速率增长于维度 $d$ 和精度倒数 $\varepsilon^{-1}$
					\item 在超立方体 $[0,1]^d$ 上, 神经网络实现的 $L^{2}$-逼近误差不超过 $\varepsilon$
				\end{enumerate}
		\end{itemize}
	\end{frame}

	\section{结论与未来展望}
	\begin{frame}{结论与未来展望}
		\begin{itemize}
			\item 利用非线性蒙特卡罗和深度学习方法, 设计出不受维数灾难影响的高效数值算法
			\item 理论与数值实验均证明: 对于控制, 金融, 量子等领域的高维问题, 这些方法具有显著优势
			\item 未来工作: 进一步完善理论证明, 改进算法效率以及扩展到更广泛的应用场景
		\end{itemize}
	\end{frame}

	\section{结束页}
	\begin{frame}{结束页}
		\centering \Huge 谢谢！
	\end{frame}

\end{document}
